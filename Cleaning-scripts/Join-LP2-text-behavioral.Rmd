---
title: "Join-LP2-BX-Text"
author: ""
date: "`r Sys.Date()`"
output:
  html_document:
    code_folding: hide
    df_print: paged
    highlight: tango
    theme: united
    toc: yes
    toc_float:
      collapsed: yes
      smooth_scroll: yes
  github_document:
    toc: true
    toc_depth: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = FALSE)
options(scipen=999)
```

# load packages
```{r}
if(!require('pacman')) {
	install.packages('pacman')
}

pacman::p_load(tidyverse, devtools, lmerTest, furrr, specr, brms, tidybayes, knitr, kableExtra, install = TRUE)

#devtools::install_github("dcosme/specr", ref = "plotmods")

if (!require(scorequaltrics)) {
  devtools::install_github('dcosme/qualtrics', ref = "dev/enhance")
}

```



# pull qualtrics data {.tabset}
## define variables and paths

To pull data from Qualtrics, you need a credentials file with an API token associated with your account. To create the file, follow these steps.

1. Generate an API token for Qualtrics. Follow the steps outlined [here](https://www.qualtrics.com/support/integrations/api-integration/overview/).

2. Save a Qualtrics credentials text file with the following format. In this example, the file is being saved as `~/credentials.yaml.PENN`. The `baseurl` is the URL for your institution on Qualtrics. Use `upenn.co1.qualtrics.com` for Penn Qualtrics.

```
token: oILNW6...[your qualtrics API token]
baseurl: upenn.co1.qualtrics.com
```

`cred_file_location` = path to your Qualtrics credential file. 

`survey_name_filter` = regular expression to filter the available surveys

```{r}
keep_columns = '(ResponseId|PROLIFIC_PID|order|Finished|Progress|no_consent|failed|)'
cred_file_location = "/Users/stevenmesquiti/Desktop/LP2-within/LP2-text-prediction/LP2-text-prediction/credentials.yaml.PENN" #update with the path to your toke
survey_name_filter = "LP2 within-person"
ignore_items = "IPAddress|RecipientFirstName|RecipientLastName|RecipientEmail|ExternalReference|LocationLatitude|LocationLongitude"
wrong_ids = "(5e711d9e908b46031e32138c|603e8ddbeaced6f573e3042d|62685fb139dc07380edf7988|62b674f342eb19fc64ed0646|6413c7853eba2b2215197ff4|5c919a87707ed900018591e3|5ef0fef359642a0b58ba5c0d|5a21ac2c681a240001ff4103|60fdc474697370c2feca0ac0|595bee9282be5500017ad683|5f106ee6b1b83d04c55aef29|56f0393f3f71990005d0e66d|5e36efb122d9f6672e1867c2|5fc44569444cef8809245f94|6310bdd0c04dd21ec4d97496|5edb6d2469290d00092100f8|5dee896cf87f5552d3cd6502|636178d7dbd1088b2445122e|5bb8740a0cb04c00014f72e9|5d230365b7ca9a00017d8919)"
```

## fetch qualtrics data
The Qualtrics API is pretty finicky. If you get the following error, just rerun the `get_survey_data` command until it works:

```
Error in qualtrics_response_codes(f, raw = TRUE) : 
  Qualtrics API complains that the requested resource cannot be found (404 error).
Please check if you are using the correct survey ID.
```

```{r}
if (!file.exists("../data/survey_data.RDS")) {
  # load credential file
  credentials = scorequaltrics::creds_from_file(cred_file_location)
  
  # filter
  surveysAvail = scorequaltrics::get_surveys()
  surveysFiltered = filter(surveysAvail, grepl(survey_name_filter, name))
  knitr::kable(arrange(select(surveysFiltered, name), name))
  
  # fetch data
  surveys_long = scorequaltrics::get_survey_data(surveysFiltered,
                                               credentials,
                                               pid_col = keep_columns) %>%
    gather(item, value, -c("survey_name", "ResponseId", "PROLIFIC_PID", "order",
                           "prompt_order", "DistributionChannel", "Finished", "Progress", "no_consent"), 
           -contains("failed"), -contains("Date"), -contains("Duration")) %>%
    filter(!grepl(ignore_items, item)) #filter out identifiable data

  saveRDS(surveys_long, "../data/survey_data.RDS")
} else {
  surveys_long = readRDS("../data/survey_data.RDS")
}
```

## tidy qualtrics data
```{r}
orders = surveys_long %>%
  filter(grepl("pre", survey_name)) %>%
  select(PROLIFIC_PID, order) %>%
  unique()

opt_out = surveys_long %>%
  filter(item == "continuation" & value == "2")

surveys_tidy = surveys_long %>%
  filter(!is.na(PROLIFIC_PID)) %>% # remove test responses
  filter(nchar(PROLIFIC_PID) > 1) %>% # remove test responses
  filter(!grepl(wrong_ids, PROLIFIC_PID)) %>%
  filter(!DistributionChannel == "Preview") %>% # remove incomplete responses
  filter(!PROLIFIC_PID %in% opt_out$PROLIFIC_PID) %>% #remove opt-out responses
  filter(!grepl("_DO_", item)) %>% # item orders
  filter(!grepl("timer", item)) %>% # timers
  filter(!grepl("UserLanguage|Status", item)) %>%
  filter(Progress >80) %>%
  select(-DistributionChannel, -order) %>%
  left_join(., orders) %>%
  extract(survey_name, c("survey_name", "study"), "LP2 within-person (.*) \\((pilot [1-2]{1})\\)") %>%
  mutate(study = ifelse(grepl("2023-07", StartDate), "full", study)) %>%
  filter(!(order == "A" & survey_name == "daily experimental 5" & grepl("07-22|07-23", RecordedDate))) # remove incorrectly pushed surveys
           
ids = surveys_tidy %>%
  select(PROLIFIC_PID) %>%
  unique() %>%
  mutate(pID = sprintf("SRS%03d", row_number())) %>%
  left_join(., unique(select(surveys_tidy, PROLIFIC_PID, ResponseId)))

missing_orders = surveys_tidy %>%
  filter(is.na(order)) %>%
  select(PROLIFIC_PID) %>%
  unique()

data = surveys_tidy %>%
  left_join(., ids) %>%
  filter(is.na(failed_screening)) %>%
  filter(!PROLIFIC_PID %in% missing_orders$PROLIFIC_PID) %>%
  filter(study %in% c("pilot 2", "full"))

data_daily = data %>%
  filter(grepl("daily", survey_name)) %>%
  filter(grepl("daily_[0-9]{1}|personal_[0-9]{1}|compassion|emotion", item)) %>%
  mutate(day = ifelse(grepl("pre", survey_name), 1,
               ifelse(grepl("post", survey_name), 14,
               ifelse(order == "A" & grepl("experimental 1", survey_name), 2,
               ifelse(order == "A" & grepl("experimental 2", survey_name), 3,
               ifelse(order == "A" & grepl("experimental 3", survey_name), 4,
               ifelse(order == "A" & grepl("control 1", survey_name), 5,
               ifelse(order == "A" & grepl("control 2", survey_name), 6,
               ifelse(order == "A" & grepl("control 3", survey_name), 7,
               ifelse(order == "A" & grepl("experimental 4", survey_name), 8,
               ifelse(order == "A" & grepl("experimental 5", survey_name), 9,
               ifelse(order == "A" & grepl("experimental 6", survey_name), 10,
               ifelse(order == "A" & grepl("control 4", survey_name), 11,
               ifelse(order == "A" & grepl("control 5", survey_name), 12,
               ifelse(order == "A" & grepl("control 6", survey_name), 13,
               ifelse(order == "B" & grepl("control 1", survey_name), 2,
               ifelse(order == "B" & grepl("control 2", survey_name), 3,
               ifelse(order == "B" & grepl("control 3", survey_name), 4,
               ifelse(order == "B" & grepl("experimental 1", survey_name), 5,
               ifelse(order == "B" & grepl("experimental 2", survey_name), 6,
               ifelse(order == "B" & grepl("experimental 3", survey_name), 7,
               ifelse(order == "B" & grepl("control 4", survey_name), 8,
               ifelse(order == "B" & grepl("control 5", survey_name), 9,
               ifelse(order == "B" & grepl("control 6", survey_name), 10,
               ifelse(order == "B" & grepl("experimental 4", survey_name), 11,
               ifelse(order == "B" & grepl("experimental 5", survey_name), 12,
               ifelse(order == "B" & grepl("experimental 6", survey_name), 13, NA))))))))))))))))))))))))))) %>%
  select(study, pID, order, survey_name, RecordedDate, day, item, value) %>%
  mutate(type = gsub("_.*", "", item),
         item = recode(item, "personal_1" = "purpose",
                       "personal_4" = "self-clarity",
                       "personal_5" = "self-acceptance",
                       "personal_6" = "self-curiosity",
                       "emotion_1" = "determination",
                       "emotion_2" = "excitement",
                       "emotion_3" = "calmness",
                       "emotion_4" = "interest",
                       "emotion_6" = "happiness",
                       "emotion_8" = "sadness",
                       "emotion_9" = "anger",
                       "emotion_10" = "stress",
                       "emotion_11" = "loneliness",
                       "emotion_12" = "depression",
                       "emotion_13" = "anxiety",
                       "compassion_1" = "self",
                       "compassion_2" = "other",
                       "daily_1" = "goal progress",
                       "daily_2" = "social connection",
                       "daily_3" = "autonomous motivation",
                       "daily_4" = "controlled motivation",
                       "daily_5" = "self-insight",
                       "daily_6" = "mindful attention",
                       "daily_7" = "mindful acceptance"),
         type = case_when(grepl("determination|excitement|calmness|interest|happiness", item) ~ "positive emotion",
                          grepl("sadness|anger|stress|loneliness|depression|anxiety", item) ~ "negative emotion",
                          grepl("motivation", item) ~ "motivation",
                          grepl("purpose|social", item) ~ "well-being",
                          grepl("self-", item) ~ "self",
                          grepl("mindful", item) ~ "mindfulness",
                          grepl("goal", item) ~ "goals",
                          grepl("^self$|other", item) ~ "compassion"),
         value = as.numeric(value),
         value = ifelse(item == "self-insight", 100-value, value)) %>% # reverse score self-insight
  extract(survey_name, c("condition", "number"), "daily (control|experimental) ([1-6]{1})", remove = FALSE) %>%
  mutate(number = as.numeric(number)) %>%
  group_by(item) %>%
  mutate(value_std = scale(value, center = TRUE, scale = TRUE))

condition_lag = data_daily %>%
  ungroup() %>%
  select(pID, day, condition) %>%
  unique() %>%
  arrange(pID, day) %>%
  group_by(pID) %>%
  mutate(condition_lag = lag(condition))

data_daily = data_daily %>%
  left_join(., condition_lag)

# data_surveys has the item level data 

data_surveys = data %>%
  filter(Finished == 1) %>%
  filter(grepl("pre|post", survey_name)) %>%
  filter(!is.na(value)) %>%
  filter(!grepl("OE|Click|Submit|Count|consent|screen|attention|therapy|age|race|ses_|state|study|continuation|personality|gender|practice|hispanic|feedback", item)) %>%
  filter(grepl("SWLS|PWB", item)) %>%
  select(PROLIFIC_PID, pID, survey_name, item, value)
```

## score survey data
```{r rubrics, results = "hide"}
# specify rubric paths
rubric_dir = here::here("scoring_rubrics")
scoring_rubrics = data.frame(file = dir(file.path('/Users/stevenmesquiti/Desktop/LP2-within/LP2-intervention-within/code/scoring_rubrics'), 
                                        pattern = '*.csv',
                                        full.names = TRUE))
# read in rubrics
scoring_data_long = scorequaltrics::get_rubrics(scoring_rubrics, type = 'scoring')

# score
scored = scorequaltrics::score_questionnaire(data_surveys, scoring_data_long, SID = "pID", psych = FALSE) %>%
  rename("pID" = SID) %>%
  mutate(score = as.numeric(score),
         measure = sprintf("%s %s", scale_name, scored_scale),
         type = ifelse(grepl("CES|GAD|ULS", scale_name), "mental health",
                ifelse(grepl("positive_emotion", measure), "positive emotion",
                ifelse(grepl("negative_emotion", measure), "negative emotion",
                ifelse(grepl("SRIS|FFMQ", scale_name), "self-insight & awareness",
                ifelse(grepl("IAF|CERQ|RRS|SCS-S", scale_name), "moderator", "well-being"))))),
         time = recode(survey_name, "pre" = 0, "post" = 1),
         survey_name = factor(survey_name, levels = c("pre", "post"))) %>%
  filter(!method == "sum") %>%
  group_by(measure) %>%
  mutate(score_std = scale(score, center = TRUE, scale = TRUE)) 

```

## spread the data long to wide 
```{r}

new_ids <- ids %>% 
  select(1,2) %>% 
  unique()

newdf <- scored %>% 
  left_join(new_ids) 

scored_wide = newdf %>%
    unite(measure, survey_name, scale_name, scored_scale, sep = "_") %>% 
    select(-n_items, -n_missing, -method,-score_std,-time,-type) %>%
    spread(measure, score)


#### spread the data_survey 


data_survey_wide <- data_surveys %>%
  unite(survey_name, survey_name,item, sep = "_") %>% 
  select(-pID) %>% 
  spread(survey_name, value)

scored_wide_1 = scored_wide %>% 
  left_join(data_survey_wide)

```


## load in the phonic data
```{r}
#read in all the CSVS with the processed data 
directory <- "/Users/stevenmesquiti/Box Sync/CurrentProjects_Penn/LP2/within_person_intervention/data/phonic-data/wave1/processed-text"
# List all CSV files in the directory
csv_files <- list.files(directory, pattern = "*.csv")

# Create a list to store the data frames
df_list <- list()

# Read each CSV file into a separate data frame and store them in the list
for (file in csv_files) {
  # Construct the full file path
  file_path <- file.path(directory, file)
  
  # Read the CSV file into a data frame and assign it a name
  df <- read.csv(file_path)
  
  # Extract the name of the data frame from the file name (excluding the ".csv" extension)
  df_name <- sub(".csv$", "", file)
  
  # Assign the data frame to a list with the same name
  df_list[[df_name]] <- df
}



# Assuming df_list is a list of data frames
# Find the number of rows in each data frame
row_counts <- sapply(df_list, nrow)

# Identify the minimum number of rows among the data frames
min_rows <- min(row_counts)

# Trim the data frames to have the same number of rows
df_list <- lapply(df_list, function(df) df[1:min_rows, ])

# Now you can safely use bind_cols
combined_df <- bind_cols(df_list)


columns_to_keep <- c("Date...1", "responseId...4", grep("Transcription$", colnames(combined_df), value = TRUE))

# Subset the data frame to keep only the selected columns
subset_combined_df <- combined_df[, columns_to_keep]

subset_combined_df <- subset_combined_df %>%
  rename(
    Date = "Date...1",
    responseId = "responseId...4"
  )

```

## Create IDs to join off of and write to csv
```{r}
ids <- ids %>% 
  select(2,3) %>% 
  rename(responseId = ResponseId) %>% 
  unique()

newdf <- subset_combined_df %>% 
  left_join(ids) 

#merge the data 

merged <- scored_wide_1 %>% 
  left_join(newdf) %>% 
  unique()

colnames(scored_wide_1)

### rename the item-level scored columns 
# Define the mapping of old column names to new subscale names with "post_" and "pre_"
subscale_mapping <- c(
  "post_PWB_1" = "post_Self-Acceptance_1",
  "post_PWB_2" = "post_Self-Acceptance_2",
  "post_PWB_3" = "post_Purpose_in_Life_1",
  "post_PWB_4" = "post_Environmental_Mastery_1",
  "post_PWB_5" = "post_Self-Acceptance_3",
  "post_PWB_6" = "post_Positive_Relations_1",
  "post_PWB_7" = "post_Purpose_in_Life_2",
  "post_PWB_8" = "post_Environmental_Mastery_2",
  "post_PWB_9" = "post_Environmental_Mastery_3",
  "post_PWB_10" = "post_Purpose_in_Life_3",
  "post_PWB_11" = "post_Personal_Growth_1",
  "post_PWB_12" = "post_Personal_Growth_2",
  "post_PWB_13" = "post_Positive_Relations_2",
  "post_PWB_14" = "post_Personal_Growth_3",
  "post_PWB_15" = "post_Autonomy_1",
  "post_PWB_16" = "post_Positive_Relations_3",
  "post_PWB_17" = "post_Autonomy_2",
  "post_PWB_18" = "post_Autonomy_3",
  "pre_PWB_1" = "pre_Self-Acceptance_1",
  "pre_PWB_2" = "pre_Self-Acceptance_2",
  "pre_PWB_3" = "pre_Purpose_in_Life_1",
  "pre_PWB_4" = "pre_Environmental_Mastery_1",
  "pre_PWB_5" = "pre_Self-Acceptance_3",
  "pre_PWB_6" = "pre_Positive_Relations_1",
  "pre_PWB_7" = "pre_Purpose_in_Life_2",
  "pre_PWB_8" = "pre_Environmental_Mastery_2",
  "pre_PWB_9" = "pre_Environmental_Mastery_3",
  "pre_PWB_10" = "pre_Purpose_in_Life_3",
  "pre_PWB_11" = "pre_Personal_Growth_1",
  "pre_PWB_12" = "pre_Personal_Growth_2",
  "pre_PWB_13" = "pre_Positive_Relations_2",
  "pre_PWB_14" = "pre_Personal_Growth_3",
  "pre_PWB_15" = "pre_Autonomy_1",
  "pre_PWB_16" = "pre_Positive_Relations_3",
  "pre_PWB_17" = "pre_Autonomy_2",
  "pre_PWB_18" = "pre_Autonomy_3"
)



reverse_score_items <- c(
  "post_PWB_1", "post_PWB_2", "post_PWB_3",
  "post_PWB_8", "post_PWB_9", "post_PWB_11",
  "post_PWB_12", "post_PWB_13", "post_PWB_17",
  "post_PWB_18",
  "pre_PWB_1", "pre_PWB_2", "pre_PWB_3",
  "pre_PWB_8", "pre_PWB_9", "pre_PWB_11",
  "pre_PWB_12", "pre_PWB_13", "pre_PWB_17",
  "pre_PWB_18"
)

# Reverse score specific items in the dataset
merged[reverse_score_items] <- lapply(merged[reverse_score_items], function(x) 8 - as.numeric(x))


merged <- merged %>%
  rename_with(.cols = all_of(names(subscale_mapping)), .fn = ~subscale_mapping[.])


write_csv(merged,"/Users/stevenmesquiti/Box Sync/CurrentProjects_Penn/LP2/within_person_intervention/data/surveys_scored/LP2_transcriptions_behavioral.csv")

```




